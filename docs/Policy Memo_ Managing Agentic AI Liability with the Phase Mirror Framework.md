# **Policy Memo: Managing Agentic AI Liability with the Phase Mirror Framework**

**TO:** Board of Directors, Senior Leadership **FROM:** Office of AI Governance & Risk **DATE:** October 26, 2023 **SUBJECT:** A Governance Framework for Managing Liability from Agentic AI Systems

\--------------------------------------------------------------------------------

### **I. PURPOSE: THE EMERGING RISK LANDSCAPE OF AGENTIC AI**

The adoption of agentic AI systems introduces a new class of liability risks that differ fundamentally from predictive models. Unlike predictive AI, where accountability rests with a human user, agentic systems act autonomously, moving liability from the user to the system designer and exposing the organization to novel risks. This memorandum establishes the "Phase Mirror" framework as our required methodology for governing these risks through explicit mechanisms, not abstract principles.

Proactive governance is a fiduciary duty. The Securities and Exchange Commission (SEC) is scrutinizing how companies disclose material AI risks, making a defensible governance architecture non-negotiable. This begins with a clear-eyed acknowledgement of the core tension between AI autonomy and enterprise governance.

### **II. THE CORE DISSONANCE: AI AUTONOMY VS. ENTERPRISE GOVERNANCE**

Effective management of agentic AI requires a clear understanding of the fundamental conflict at its heart. This is not a problem to be solved, but a permanent tension that must be managed through explicit bindings. Agentic AI promises autonomous decision-making and action; enterprise governance demands deterministic guarantees, audit trails, and human accountability.

These forces are in opposition. The Phase Mirror methodology confronts this conflict by replacing vague "vibe claims" with concrete actions and routing abstract complaints to specific mechanisms: **issuance, access, incentives, and liability.** Instead of aspiring to "align" AI, we will build systems that bind its behavior to our policies, allowing us to identify the specific liability exposures this core tension creates.

### **III. ANALYSIS OF KEY LIABILITY EXPOSURES**

Applying the Phase Mirror framework to agentic systems reveals predictable dissonances. These are not system bugs; they are structural conflicts that represent the primary sources of our legal and operational liability.

* **Probabilistic Outputs vs. Binary Compliance:** Agentic systems produce probabilistic outputs based on Bayesian logic. Our legal and compliance frameworks demand binary, yes/no adherence. This friction is amplified in machine-to-machine decision chains, complicating traditional liability models.  
* **System Autonomy vs. Deterministic Success:** An agent’s capacity for autonomous action implies a "permission to fail" as it learns. This is in direct conflict with the enterprise’s demand for deterministic success in high-stakes environments, requiring tiered governance models (human-in-the-loop vs. human-on-the-loop) to manage unacceptable risk.  
* **"Reasoning" Capability vs. Data Integrity:** An agent's ability to perform causal reasoning exposes previously hidden gaps in our data hygiene. Faulty, biased, or incomplete data leads an agent to generate wrongful conclusions, creating direct liability for its actions.  
* **"No-Code" Interfaces vs. Causal Expertise:** The accessibility of "no-code" tools masks the deep expertise required to build valid causal models. This creates a significant risk of employees with insufficient expertise building agents that produce faulty, liability-creating conclusions.  
* **Transparency Rhetoric vs. Proprietary Systems:** Our reliance on proprietary, third-party platforms can create "black box" scenarios where an agent's decision logic is opaque. This prevents accountability, makes audits impossible, and renders the agent's actions indefensible.

These tensions are not independent; they converge on a single, structural tradeoff that cannot be delegated to engineering. It is a governance decision that leadership must make explicitly.

### **IV. THE CRITICAL POLICY DECISION: THE COMPLIANCE-ACCURACY TRADEOFF**

The choice between optimizing for the most compliant outcome or the most accurate one is not a system failure; it is a structural decision requiring explicit policy. The Phase Mirror methodology demands this tradeoff be named plainly. This is not a technical configuration; it is the defining policy choice that will dictate our liability architecture.

The Precision Question we must answer is: **Does the agent optimize for the most accurate outcome or the most compliant one?**

This question cannot have an abstract answer. The answer must be encoded as formal policy with explicit triggers defining when compliance overrides accuracy. When an agent's "reasoning" conflicts with its "rules," its behavior must be predetermined. The system must not freeze or hallucinate a compliant answer. It must escalate the conflict to a pre-defined human exception handler, accompanied by a logged rationale.

### **V. RECOMMENDED GOVERNANCE MECHANISMS & LEVERS**

To enforce this policy, the following governance levers will be implemented immediately.

| Owner | Proposed Lever | Key Metric | Horizon |
| :---- | :---- | :---- | :---- |
| Governance | Define a formal "error budget" to set acceptable failure rates for agentic decisions. | Allowable Failure Rate % | 30 days |
| Legal | Map probabilistic outputs to defined corporate liability tiers. | Audit Pass Rate | 60 days |
| Ops | Evolve oversight from "human-in-the-loop" to "human-on-exception" to scale monitoring. | Intervention Ratio | 45 days |
| Eng | Implement and stress-test agent autonomy against hard-coded safety guardrails. | Violation Rate 0% | 21 days |
| Data | Quantify causal model validity score *before* any production deployment. | Graph Accuracy Score | 14 days |

These levers are not suggestions; they become binding through the following governance artifacts.

### **VI. IMPLEMENTATION: BINDING GOVERNANCE TO ARTIFACTS**

Effective governance moves from abstract alignment to concrete artifacts. The Phase Mirror heuristic is our mandate: replace the verb "align" with an artifact. Therefore, the following are non-negotiable implementation requirements for any high-risk agentic AI system.

* **`Spec`:** A formal document defining what constitutes an "acceptable" error versus a policy violation.  
* **`Contract`:** Clear contracts establishing liability tiers between our organization and any third-party platform providers.  
* **`SLA`:** Service Level Agreements that commit to specific audit pass rates and explainability standards.  
* **`Dataset`:** A mandatory requirement for causal model validity scoring *before* any production deployment.  
* **`Kill-Switch`:** Governance triggers and rollback mechanisms for any agentic plan that depends on hope or operates in a high-stakes environment.

### **VII. CONCLUSION: MANAGE TENSIONS, DON'T SOLVE CONTRADICTIONS**

The contradictions between AI autonomy and enterprise governance are not bugs to be fixed. They are permanent tensions to be explicitly managed. Our goal is not to solve them, but to govern them with rigor.

This framework provides the necessary tools to govern this tension. Our mandate is to use them, ensuring innovation is bound by discipline and liability is managed by design, not by hope.

